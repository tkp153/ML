{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1670588587469,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"3VfEcVSNPWiL","outputId":"981789c2-d8b6-497f-9b19-bcb7d5fbb789"},"outputs":[],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5131,"status":"ok","timestamp":1670588592590,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"xauIBijEPjJO","outputId":"f9e77cbd-7847-49e0-e4b8-947a33f01721"},"outputs":[],"source":["!python -c 'import torch; print(torch.__version__) '"]},{"cell_type":"markdown","metadata":{"id":"KndWvALzfoG_"},"source":["# YOLOX 依存パッケージインストール(YOLOX Dependent Package Install)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1794,"status":"ok","timestamp":1670588594379,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"YpviPxKHfh59","outputId":"48268e75-bd39-4d9c-a8d2-30bb3657cfa8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'YOLOX'...\n","remote: Enumerating objects: 1764, done.\u001b[K\n","remote: Counting objects: 100% (41/41), done.\u001b[K\n","remote: Compressing objects: 100% (35/35), done.\u001b[K\n","remote: Total 1764 (delta 13), reused 19 (delta 4), pack-reused 1723\u001b[K\n","Receiving objects: 100% (1764/1764), 6.85 MiB | 1.67 MiB/s, done.\n","Resolving deltas: 100% (1038/1038), done.\n"]}],"source":["!git clone https://github.com/Megvii-BaseDetection/YOLOX"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42565,"status":"ok","timestamp":1670588636935,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"maY7_U3gLhQA","outputId":"f215a577-ceee-434c-af90-9eca9000051d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/machinelearning/vscode/ML/ML-1/YOLOX\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: pip in /home/machinelearning/.local/lib/python3.10/site-packages (22.3.1)\n","Defaulting to user installation because normal site-packages is not writeable\n","Collecting numpy\n","  Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","Collecting torch>=1.7\n","  Using cached torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n","Collecting opencv_python\n","  Using cached opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n","Collecting loguru\n","  Using cached loguru-0.6.0-py3-none-any.whl (58 kB)\n","Collecting tqdm\n","  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n","Collecting torchvision\n","  Using cached torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n","Collecting thop\n","  Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Collecting ninja\n","  Using cached ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n","Collecting tabulate\n","  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n","Collecting pycocotools>=2.0.2\n","  Using cached pycocotools-2.0.6.tar.gz (24 kB)\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting onnx==1.8.1\n","  Using cached onnx-1.8.1.tar.gz (5.2 MB)\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m \u001b[31m[18 lines of output]\u001b[0m\n","  \u001b[31m   \u001b[0m fatal: not a git repository (or any of the parent directories): .git\n","  \u001b[31m   \u001b[0m Traceback (most recent call last):\n","  \u001b[31m   \u001b[0m   File \"/home/machinelearning/.local/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 351, in <module>\n","  \u001b[31m   \u001b[0m     main()\n","  \u001b[31m   \u001b[0m   File \"/home/machinelearning/.local/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 333, in main\n","  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n","  \u001b[31m   \u001b[0m   File \"/home/machinelearning/.local/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n","  \u001b[31m   \u001b[0m     return hook(config_settings)\n","  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-lq1a1n4r/overlay/local/lib/python3.10/dist-packages/setuptools/build_meta.py\", line 338, in get_requires_for_build_wheel\n","  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=['wheel'])\n","  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-lq1a1n4r/overlay/local/lib/python3.10/dist-packages/setuptools/build_meta.py\", line 320, in _get_build_requires\n","  \u001b[31m   \u001b[0m     self.run_setup()\n","  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-lq1a1n4r/overlay/local/lib/python3.10/dist-packages/setuptools/build_meta.py\", line 484, in run_setup\n","  \u001b[31m   \u001b[0m     super(_BuildMetaLegacyBackend,\n","  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-lq1a1n4r/overlay/local/lib/python3.10/dist-packages/setuptools/build_meta.py\", line 335, in run_setup\n","  \u001b[31m   \u001b[0m     exec(code, locals())\n","  \u001b[31m   \u001b[0m   File \"<string>\", line 75, in <module>\n","  \u001b[31m   \u001b[0m AssertionError: Could not find \"cmake\" executable!\n","  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","Using pip 22.3.1 from /home/machinelearning/.local/lib/python3.10/site-packages/pip (python 3.10)\n","Defaulting to user installation because normal site-packages is not writeable\n","Obtaining file:///home/machinelearning/vscode/ML/ML-1/YOLOX\n","  Preparing metadata (setup.py) ... \u001b[?25l  Running command python setup.py egg_info\n","  [WARNING] Unable to import torch, pre-compiling ops will be disabled.\n","  Traceback (most recent call last):\n","    File \"<string>\", line 2, in <module>\n","    File \"<pip-setuptools-caller>\", line 34, in <module>\n","    File \"/home/machinelearning/vscode/ML/ML-1/YOLOX/setup.py\", line 77, in <module>\n","      ext_modules=get_ext_modules(),\n","    File \"/home/machinelearning/vscode/ML/ML-1/YOLOX/setup.py\", line 50, in get_ext_modules\n","      assert TORCH_AVAILABLE, \"torch is required for pre-compiling ops, please install it first.\"\n","  AssertionError: torch is required for pre-compiling ops, please install it first.\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  \u001b[1;35mfull command\u001b[0m: \u001b[34m/usr/bin/python3 -c '\u001b[0m\n","\u001b[34m  exec(compile('\"'\"''\"'\"''\"'\"'\u001b[0m\n","\u001b[34m  # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\u001b[0m\n","\u001b[34m  #\u001b[0m\n","\u001b[34m  # - It imports setuptools before invoking setup.py, to enable projects that directly\u001b[0m\n","\u001b[34m  #   import from `distutils.core` to work with newer packaging standards.\u001b[0m\n","\u001b[34m  # - It provides a clear error message when setuptools is not installed.\u001b[0m\n","\u001b[34m  # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so\u001b[0m\n","\u001b[34m  #   setuptools doesn'\"'\"'t think the script is `-c`. This avoids the following warning:\u001b[0m\n","\u001b[34m  #     manifest_maker: standard file '\"'\"'-c'\"'\"' not found\".\u001b[0m\n","\u001b[34m  # - It generates a shim setup.py, for handling setup.cfg-only projects.\u001b[0m\n","\u001b[34m  import os, sys, tokenize\u001b[0m\n","\u001b[34m  \u001b[0m\n","\u001b[34m  try:\u001b[0m\n","\u001b[34m      import setuptools\u001b[0m\n","\u001b[34m  except ImportError as error:\u001b[0m\n","\u001b[34m      print(\u001b[0m\n","\u001b[34m          \"ERROR: Can not execute `setup.py` since setuptools is not available in \"\u001b[0m\n","\u001b[34m          \"the build environment.\",\u001b[0m\n","\u001b[34m          file=sys.stderr,\u001b[0m\n","\u001b[34m      )\u001b[0m\n","\u001b[34m      sys.exit(1)\u001b[0m\n","\u001b[34m  \u001b[0m\n","\u001b[34m  __file__ = %r\u001b[0m\n","\u001b[34m  sys.argv[0] = __file__\u001b[0m\n","\u001b[34m  \u001b[0m\n","\u001b[34m  if os.path.exists(__file__):\u001b[0m\n","\u001b[34m      filename = __file__\u001b[0m\n","\u001b[34m      with tokenize.open(__file__) as f:\u001b[0m\n","\u001b[34m          setup_py_code = f.read()\u001b[0m\n","\u001b[34m  else:\u001b[0m\n","\u001b[34m      filename = \"<auto-generated setuptools caller>\"\u001b[0m\n","\u001b[34m      setup_py_code = \"from setuptools import setup; setup()\"\u001b[0m\n","\u001b[34m  \u001b[0m\n","\u001b[34m  exec(compile(setup_py_code, filename, \"exec\"))\u001b[0m\n","\u001b[34m  '\"'\"''\"'\"''\"'\"' % ('\"'\"'/home/machinelearning/vscode/ML/ML-1/YOLOX/setup.py'\"'\"',), \"<pip-setuptools-caller>\", \"exec\"))' egg_info --egg-base /tmp/pip-pip-egg-info-hw53myyc\u001b[0m\n","  \u001b[1;35mcwd\u001b[0m: /home/machinelearning/vscode/ML/ML-1/YOLOX/\n","\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}],"source":["%cd YOLOX\n","\n","!pip install -U pip && pip install -r requirements.txt\n","!pip install -v -e .  "]},{"cell_type":"markdown","metadata":{"id":"k1t1Hb74iIZo"},"source":["# PyCocoToolsインストール(PyCocoTools Install)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18665,"status":"ok","timestamp":1670588655593,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"YCje1A8mhtLy","outputId":"e290c84b-2fab-4b44-f2d6-3ac9d0cb7e3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: cython in /home/machinelearning/.local/lib/python3.10/site-packages (0.29.32)\n","Defaulting to user installation because normal site-packages is not writeable\n","Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n","  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-u_2p3lon\n","  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-u_2p3lon\n","  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n","  Preparing metadata (setup.py) ... \u001b[?25lerror\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n","  \u001b[31m   \u001b[0m Traceback (most recent call last):\n","  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n","  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n","  \u001b[31m   \u001b[0m   File \"/tmp/pip-req-build-u_2p3lon/PythonAPI/setup.py\", line 2, in <module>\n","  \u001b[31m   \u001b[0m     import numpy as np\n","  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'numpy'\n","  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}],"source":["!pip install cython\n","!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"]},{"cell_type":"markdown","metadata":{"id":"iWp26peevulP"},"source":["# データセットダウンロード(Download Dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61733,"status":"ok","timestamp":1670588717286,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"NSZzrY_ZPJZV","outputId":"764c71ce-311e-4453-8602-9a060695b974"},"outputs":[],"source":["\"\"\"from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":438,"status":"ok","timestamp":1670588717703,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"XZQxPDFFPLas"},"outputs":[],"source":["!cp /content/drive/MyDrive/datav5.zip ./"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":380,"status":"ok","timestamp":1670588718076,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"KqJJ2qOKPPzh","outputId":"b22b2b23-ea11-4364-d79a-c9a6b6f38c45"},"outputs":[],"source":["!unzip datav5.zip"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1670588718077,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"frzsMeeetO1y","outputId":"6b792e21-4319-4148-e5c2-a5d18463c3bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/machinelearning/vscode/ML/ML-1\n"]}],"source":["%cd /home/machinelearning/vscode/ML/ML-1\n","use_sample_image = False\n","\n","if use_sample_image:\n","    !git clone https://github.com/Kazuhito00/YOLOX-Colaboratory-Training-Sample.git"]},{"cell_type":"markdown","metadata":{"id":"NC3Frlnzz5eC"},"source":["# 学習/検証データ分割(Train/Validation split data)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1670588718077,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"pkp7yRJPv0_1"},"outputs":[],"source":["import os\n","\n","# 独自のデータを使用する場合は、パスを指定してください\n","# Please fill in the path if you want to use your own data\n","if use_sample_image:\n","    dataset_directory = 'YOLOX-Colaboratory-Training-Sample/02.annotation_data'\n","else:\n","    dataset_directory = './data'\n","\n","# 学習/検証データパス(train/validation data path)\n","train_directory = './train'\n","validation_directory = './validation'\n","\n","# 学習データ格納ディレクトリ作成(Create training data storage directory)\n","os.makedirs(train_directory, exist_ok=True)\n","# 検証データ格納ディレクトリ作成(Create verification data storage directory)\n","os.makedirs(validation_directory, exist_ok=True)"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1670588718078,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"BJieAq9IywqQ"},"outputs":[],"source":["import glob\n","import shutil\n","import random\n","\n","# 学習データの割合(Percentage of training data)\n","train_ratio = 0.8\n","\n","# コピー元ファイルリスト取得(Get copy source file list)\n","annotation_list = sorted(glob.glob(dataset_directory + '/*.xml'))\n","image_list = sorted(glob.glob(dataset_directory + '/*.jpg'))\n","\n","file_num = len(annotation_list)\n","\n","# インデックスシャッフル(shuffle)\n","index_list = list(range(file_num - 1))\n","random.shuffle(index_list)\n","\n","for count, index in enumerate(index_list):\n","    if count < int(file_num * train_ratio):\n","        # 学習用データ(Training Data)\n","        shutil.copy2(annotation_list[index], train_directory)\n","        shutil.copy2(image_list[index], train_directory)\n","    else:\n","        # 検証用データ(Validation Data)\n","        shutil.copy2(annotation_list[index], validation_directory)\n","        shutil.copy2(image_list[index], validation_directory)"]},{"cell_type":"markdown","metadata":{"id":"ACKapHgx_d4Q"},"source":["# Pascal VOC形式 を MS COCO形式へ変換(Convert Pascal VOC format to MS COCO format)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1031,"status":"ok","timestamp":1670588719103,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"qKGpaUik_c9m","outputId":"7b44b885-06e7-4840-fdfc-20a8eb7286d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'convert_voc_to_coco'...\n","remote: Enumerating objects: 27, done.\u001b[K\n","remote: Counting objects: 100% (27/27), done.\u001b[K\n","remote: Compressing objects: 100% (20/20), done.\u001b[K\n","remote: Total 27 (delta 12), reused 14 (delta 6), pack-reused 0\u001b[K\n","Receiving objects: 100% (27/27), 11.77 KiB | 148.00 KiB/s, done.\n","Resolving deltas: 100% (12/12), done.\n"]}],"source":["!git clone https://github.com/Kazuhito00/convert_voc_to_coco.git"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":452,"status":"ok","timestamp":1670588719518,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"t3xTEz30_kYp","outputId":"1b8bebd1-68d5-4686-df92-95f90417b065"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of xml files: 226\n","Convert XML to JSON: 100%|█████████████████| 226/226 [00:00<00:00, 34220.68it/s]\n","{'handle_folk': 106, 'top_folk': 116}\n","Success: train/train_annotations.json\n","Number of xml files: 56\n","Convert XML to JSON: 100%|███████████████████| 56/56 [00:00<00:00, 29316.15it/s]\n","{'handle_folk': 30, 'top_folk': 24}\n","Success: validation/validation_annotations.json\n"]}],"source":["!python3 convert_voc_to_coco/convert_voc_to_coco.py \\\n","    train train/train_annotations.json \\\n","    --start_image_id=0\n","!python3 convert_voc_to_coco/convert_voc_to_coco.py \\\n","    validation validation/validation_annotations.json \\\n","    --start_image_id=10000000"]},{"cell_type":"markdown","metadata":{"id":"wJ9ytPB90pJP"},"source":["# 学習データディレクトリ準備(Training data directory preparation)"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":930,"status":"ok","timestamp":1670588720433,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"IccyvWRpDZGL"},"outputs":[],"source":["!mkdir dataset\n","!mkdir dataset/images\n","!mkdir dataset/images/train2017\n","!mkdir dataset/images/val2017\n","!mkdir dataset/annotations\n","\n","!cp -rf train/*.jpg dataset/images/train2017\n","!cp -rf validation/*.jpg dataset/images/val2017\n","!cp -rf train/train_annotations.json dataset/annotations\n","!cp -rf validation/validation_annotations.json dataset/annotations"]},{"cell_type":"markdown","metadata":{"id":"CnUirebA1a__"},"source":["# コンフィグコピー\n","<!--\n","![image](https://user-images.githubusercontent.com/37477845/135283504-254ea817-345e-4665-828a-4c6034645ed1.png)\n","-->\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1670588720434,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"gzlWZMuSPUly"},"outputs":[],"source":["if use_sample_image:\n","    !cp /content/YOLOX-Colaboratory-Training-Sample/03.config/nano.py /content/YOLOX"]},{"cell_type":"markdown","metadata":{"id":"vVvBXq4e2ydb"},"source":["# モデル訓練"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1670588720435,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"hobMWbgPUUrq","outputId":"95755616-e410-4056-9245-9bb5171031f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/machinelearning/vscode/ML/ML-1/YOLOX\n"]}],"source":["%cd YOLOX"]},{"cell_type":"markdown","metadata":{"id":"MuoY2aHcf04N"},"source":["yolox_s"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1670588789334,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"lJEIt55Tf0iX","outputId":"bf730010-d692-453a-eef1-1ebc830f359e"},"outputs":[],"source":["%%writefile yolox_s.py\n","#!/usr/bin/env python3\n","# -*- coding:utf-8 -*-\n","# Copyright (c) Megvii, Inc. and its affiliates.\n","\n","import os\n","\n","from yolox.exp import Exp as MyExp\n","\n","\n","class Exp(MyExp):\n","    def __init__(self):\n","        super(Exp, self).__init__()\n","        self.depth = 0.33\n","        self.width = 0.50\n","        self.max_epoch = 20\n","        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(\".\")[0]\n","\n","        self.data_dir = \"/content/dataset/images\"\n","        self.train_ann = \"/content/dataset/annotations/train_annotations.json\"\n","        self.val_ann = \"/content/dataset/annotations/validation_annotations.json\"\n","\n","        self.num_classes = 2\n","\n","        self.eval_interval = 1\n","\n"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6201,"status":"ok","timestamp":1670588798983,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"ykzClTsh1ZDA","outputId":"299d051c-68b3-47c9-9a9b-19d63fa915ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/YOLOX/'\n","/home/machinelearning/vscode/ML/ML-1/YOLOX\n","--2022-12-16 14:09:33--  https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth\n","Resolving github.com (github.com)... 20.27.177.113\n","Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/388351473/0b307dd4-bddb-4cfe-a863-1d19afb5598a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221216%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221216T050933Z&X-Amz-Expires=300&X-Amz-Signature=988a945486c528a7d1554b6a95c8fd085334c2cba5a1e87020b99a946f72b144&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=388351473&response-content-disposition=attachment%3B%20filename%3Dyolox_s.pth&response-content-type=application%2Foctet-stream [following]\n","--2022-12-16 14:09:33--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/388351473/0b307dd4-bddb-4cfe-a863-1d19afb5598a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221216%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221216T050933Z&X-Amz-Expires=300&X-Amz-Signature=988a945486c528a7d1554b6a95c8fd085334c2cba5a1e87020b99a946f72b144&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=388351473&response-content-disposition=attachment%3B%20filename%3Dyolox_s.pth&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 72050245 (69M) [application/octet-stream]\n","Saving to: ‘yolox_s.pth’\n","\n","yolox_s.pth         100%[===================>]  68.71M  1.74MB/s    in 50s     \n","\n","2022-12-16 14:10:24 (1.38 MB/s) - ‘yolox_s.pth’ saved [72050245/72050245]\n","\n"]}],"source":["%cd /content/YOLOX/\n","!wget https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":258344,"status":"ok","timestamp":1670589058318,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"qZDXBpaY22lN","outputId":"ff90b23d-9147-4e9a-c144-2c049c3ea0ef"},"outputs":[],"source":["!python3 tools/train.py \\\n","    -f yolox_s.py \\\n","    -d 1 \\\n","    -b 16 \\\n","    --fp16 \\\n","    -o \\\n","    -c yolox_s.pth"]},{"cell_type":"markdown","metadata":{"id":"mmlvTpsheyQm"},"source":["# 推論テスト(Inference test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54,"status":"ok","timestamp":1670589058318,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"7hz1AF1jrXJg","outputId":"4d75b46d-e5f3-413f-e882-51cbcb275e82"},"outputs":[],"source":["%cd YOLOX/tools"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1670589058318,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"vK8Ikghepwnw","outputId":"1e09626c-1686-4d18-824b-8cb65cc4a5bd"},"outputs":[],"source":["%%writefile demo.py\n","#!/usr/bin/env python3\n","# -*- coding:utf-8 -*-\n","# Copyright (c) Megvii, Inc. and its affiliates.\n","\n","import argparse\n","import os\n","import time\n","from loguru import logger\n","\n","import cv2\n","\n","import torch\n","\n","from yolox.data.data_augment import ValTransform\n","#from yolox.data.datasets import COCO_CLASSES #ここをコメントアウト\n","from yolox.exp import get_exp\n","from yolox.utils import fuse_model, get_model_info, postprocess, vis\n","\n","COCO_CLASSES = ('back','front') #ここに追記\n","\n","IMAGE_EXT = [\".jpg\", \".jpeg\", \".webp\", \".bmp\", \".png\"]\n","\n","\n","def make_parser():\n","    parser = argparse.ArgumentParser(\"YOLOX Demo!\")\n","    parser.add_argument(\n","        \"demo\", default=\"image\", help=\"demo type, eg. image, video and webcam\"\n","    )\n","    parser.add_argument(\"-expn\", \"--experiment-name\", type=str, default=None)\n","    parser.add_argument(\"-n\", \"--name\", type=str, default=None, help=\"model name\")\n","\n","    parser.add_argument(\n","        \"--path\", default=\"./assets/dog.jpg\", help=\"path to images or video\"\n","    )\n","    parser.add_argument(\"--camid\", type=int, default=0, help=\"webcam demo camera id\")\n","    parser.add_argument(\n","        \"--save_result\",\n","        action=\"store_true\",\n","        help=\"whether to save the inference result of image/video\",\n","    )\n","\n","    # exp file\n","    parser.add_argument(\n","        \"-f\",\n","        \"--exp_file\",\n","        default=None,\n","        type=str,\n","        help=\"please input your experiment description file\",\n","    )\n","    parser.add_argument(\"-c\", \"--ckpt\", default=None, type=str, help=\"ckpt for eval\")\n","    parser.add_argument(\n","        \"--device\",\n","        default=\"cpu\",\n","        type=str,\n","        help=\"device to run our model, can either be cpu or gpu\",\n","    )\n","    parser.add_argument(\"--conf\", default=0.3, type=float, help=\"test conf\")\n","    parser.add_argument(\"--nms\", default=0.3, type=float, help=\"test nms threshold\")\n","    parser.add_argument(\"--tsize\", default=None, type=int, help=\"test img size\")\n","    parser.add_argument(\n","        \"--fp16\",\n","        dest=\"fp16\",\n","        default=False,\n","        action=\"store_true\",\n","        help=\"Adopting mix precision evaluating.\",\n","    )\n","    parser.add_argument(\n","        \"--legacy\",\n","        dest=\"legacy\",\n","        default=False,\n","        action=\"store_true\",\n","        help=\"To be compatible with older versions\",\n","    )\n","    parser.add_argument(\n","        \"--fuse\",\n","        dest=\"fuse\",\n","        default=False,\n","        action=\"store_true\",\n","        help=\"Fuse conv and bn for testing.\",\n","    )\n","    parser.add_argument(\n","        \"--trt\",\n","        dest=\"trt\",\n","        default=False,\n","        action=\"store_true\",\n","        help=\"Using TensorRT model for testing.\",\n","    )\n","    return parser\n","\n","\n","def get_image_list(path):\n","    image_names = []\n","    for maindir, subdir, file_name_list in os.walk(path):\n","        for filename in file_name_list:\n","            apath = os.path.join(maindir, filename)\n","            ext = os.path.splitext(apath)[1]\n","            if ext in IMAGE_EXT:\n","                image_names.append(apath)\n","    return image_names\n","\n","\n","class Predictor(object):\n","    def __init__(\n","        self,\n","        model,\n","        exp,\n","        cls_names=COCO_CLASSES,\n","        trt_file=None,\n","        decoder=None,\n","        device=\"cpu\",\n","        fp16=False,\n","        legacy=False,\n","    ):\n","        self.model = model\n","        self.cls_names = cls_names\n","        self.decoder = decoder\n","        self.num_classes = exp.num_classes\n","        self.confthre = exp.test_conf\n","        self.nmsthre = exp.nmsthre\n","        self.test_size = exp.test_size\n","        self.device = device\n","        self.fp16 = fp16\n","        self.preproc = ValTransform(legacy=legacy)\n","        if trt_file is not None:\n","            from torch2trt import TRTModule\n","\n","            model_trt = TRTModule()\n","            model_trt.load_state_dict(torch.load(trt_file))\n","\n","            x = torch.ones(1, 3, exp.test_size[0], exp.test_size[1]).cuda()\n","            self.model(x)\n","            self.model = model_trt\n","\n","    def inference(self, img):\n","        img_info = {\"id\": 0}\n","        if isinstance(img, str):\n","            img_info[\"file_name\"] = os.path.basename(img)\n","            img = cv2.imread(img)\n","        else:\n","            img_info[\"file_name\"] = None\n","\n","        height, width = img.shape[:2]\n","        img_info[\"height\"] = height\n","        img_info[\"width\"] = width\n","        img_info[\"raw_img\"] = img\n","\n","        ratio = min(self.test_size[0] / img.shape[0], self.test_size[1] / img.shape[1])\n","        img_info[\"ratio\"] = ratio\n","\n","        img, _ = self.preproc(img, None, self.test_size)\n","        img = torch.from_numpy(img).unsqueeze(0)\n","        img = img.float()\n","        if self.device == \"gpu\":\n","            img = img.cuda()\n","            if self.fp16:\n","                img = img.half()  # to FP16\n","\n","        with torch.no_grad():\n","            t0 = time.time()\n","            outputs = self.model(img)\n","            if self.decoder is not None:\n","                outputs = self.decoder(outputs, dtype=outputs.type())\n","            outputs = postprocess(\n","                outputs, self.num_classes, self.confthre,\n","                self.nmsthre, class_agnostic=True\n","            )\n","            logger.info(\"Infer time: {:.4f}s\".format(time.time() - t0))\n","        return outputs, img_info\n","\n","    def visual(self, output, img_info, cls_conf=0.35):\n","        ratio = img_info[\"ratio\"]\n","        img = img_info[\"raw_img\"]\n","        if output is None:\n","            return img\n","        output = output.cpu()\n","\n","        bboxes = output[:, 0:4]\n","\n","        # preprocessing: resize\n","        bboxes /= ratio\n","\n","        cls = output[:, 6]\n","        scores = output[:, 4] * output[:, 5]\n","\n","        vis_res = vis(img, bboxes, scores, cls, cls_conf, self.cls_names)\n","        return vis_res\n","\n","\n","def image_demo(predictor, vis_folder, path, current_time, save_result):\n","    if os.path.isdir(path):\n","        files = get_image_list(path)\n","    else:\n","        files = [path]\n","    files.sort()\n","    for image_name in files:\n","        outputs, img_info = predictor.inference(image_name)\n","        result_image = predictor.visual(outputs[0], img_info, predictor.confthre)\n","        if save_result:\n","            save_folder = os.path.join(\n","                vis_folder, time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n","            )\n","            os.makedirs(save_folder, exist_ok=True)\n","            save_file_name = os.path.join(save_folder, os.path.basename(image_name))\n","            logger.info(\"Saving detection result in {}\".format(save_file_name))\n","            cv2.imwrite(save_file_name, result_image)\n","        ch = cv2.waitKey(0)\n","        if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n","            break\n","\n","\n","def imageflow_demo(predictor, vis_folder, current_time, args):\n","    cap = cv2.VideoCapture(args.path if args.demo == \"video\" else args.camid)\n","    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  # float\n","    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    if args.save_result:\n","        save_folder = os.path.join(\n","            vis_folder, time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n","        )\n","        os.makedirs(save_folder, exist_ok=True)\n","        if args.demo == \"video\":\n","            save_path = os.path.join(save_folder, os.path.basename(args.path))\n","        else:\n","            save_path = os.path.join(save_folder, \"camera.mp4\")\n","        logger.info(f\"video save_path is {save_path}\")\n","        vid_writer = cv2.VideoWriter(\n","            save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (int(width), int(height))\n","        )\n","    while True:\n","        ret_val, frame = cap.read()\n","        if ret_val:\n","            outputs, img_info = predictor.inference(frame)\n","            result_frame = predictor.visual(outputs[0], img_info, predictor.confthre)\n","            if args.save_result:\n","                vid_writer.write(result_frame)\n","            else:\n","                cv2.namedWindow(\"yolox\", cv2.WINDOW_NORMAL)\n","                cv2.imshow(\"yolox\", result_frame)\n","            ch = cv2.waitKey(1)\n","            if ch == 27 or ch == ord(\"q\") or ch == ord(\"Q\"):\n","                break\n","        else:\n","            break\n","\n","\n","def main(exp, args):\n","    if not args.experiment_name:\n","        args.experiment_name = exp.exp_name\n","\n","    file_name = os.path.join(exp.output_dir, args.experiment_name)\n","    os.makedirs(file_name, exist_ok=True)\n","\n","    vis_folder = None\n","    if args.save_result:\n","        vis_folder = os.path.join(file_name, \"vis_res\")\n","        os.makedirs(vis_folder, exist_ok=True)\n","\n","    if args.trt:\n","        args.device = \"gpu\"\n","\n","    logger.info(\"Args: {}\".format(args))\n","\n","    if args.conf is not None:\n","        exp.test_conf = args.conf\n","    if args.nms is not None:\n","        exp.nmsthre = args.nms\n","    if args.tsize is not None:\n","        exp.test_size = (args.tsize, args.tsize)\n","\n","    model = exp.get_model()\n","    logger.info(\"Model Summary: {}\".format(get_model_info(model, exp.test_size)))\n","\n","    if args.device == \"gpu\":\n","        model.cuda()\n","        if args.fp16:\n","            model.half()  # to FP16\n","    model.eval()\n","\n","    if not args.trt:\n","        if args.ckpt is None:\n","            ckpt_file = os.path.join(file_name, \"best_ckpt.pth\")\n","        else:\n","            ckpt_file = args.ckpt\n","        logger.info(\"loading checkpoint\")\n","        ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n","        # load the model state dict\n","        model.load_state_dict(ckpt[\"model\"])\n","        logger.info(\"loaded checkpoint done.\")\n","\n","    if args.fuse:\n","        logger.info(\"\\tFusing model...\")\n","        model = fuse_model(model)\n","\n","    if args.trt:\n","        assert not args.fuse, \"TensorRT model is not support model fusing!\"\n","        trt_file = os.path.join(file_name, \"model_trt.pth\")\n","        assert os.path.exists(\n","            trt_file\n","        ), \"TensorRT model is not found!\\n Run python3 tools/trt.py first!\"\n","        model.head.decode_in_inference = False\n","        decoder = model.head.decode_outputs\n","        logger.info(\"Using TensorRT to inference\")\n","    else:\n","        trt_file = None\n","        decoder = None\n","\n","    predictor = Predictor(\n","        model, exp, COCO_CLASSES, trt_file, decoder,\n","        args.device, args.fp16, args.legacy,\n","    )\n","    current_time = time.localtime()\n","    if args.demo == \"image\":\n","        image_demo(predictor, vis_folder, args.path, current_time, args.save_result)\n","    elif args.demo == \"video\" or args.demo == \"webcam\":\n","        imageflow_demo(predictor, vis_folder, current_time, args)\n","\n","\n","if __name__ == \"__main__\":\n","    args = make_parser().parse_args()\n","    exp = get_exp(args.exp_file, args.name)\n","\n","    main(exp, args)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670589058319,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"wUWDMRpPrjSM","outputId":"2e173532-d0b6-4149-d711-27ec3ee18c5c"},"outputs":[],"source":["%cd YOLOX"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8039,"status":"ok","timestamp":1670589289114,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"w3g0ZRUwMP8k","outputId":"d349269e-653b-4b7d-be2d-7ab362b7da6a"},"outputs":[],"source":["TEST_IMAGE_PATH = \"/content/drive/MyDrive/TestImages/南西v3.jpg\"\n","MODEL_PATH = \"/content/YOLOX/YOLOX_outputs/yolox_s/best_ckpt.pth\"\n","\n","!python tools/demo.py image \\\n","    -f yolox_s.py \\\n","    -c {MODEL_PATH} \\\n","    --path {TEST_IMAGE_PATH} \\\n","    --conf 0.25 \\\n","    --nms 0.45 \\\n","    --tsize 416 \\\n","    --save_result \\\n","    --device gpu"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488,"output_embedded_package_id":"17P5uaIcQDFjaVPZbBQoWAmHrOX8Ws-9V"},"executionInfo":{"elapsed":11462,"status":"ok","timestamp":1670589321374,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"67bW5EWTtUA-","outputId":"4377084e-3e01-4673-9f31-5922ab58042e"},"outputs":[],"source":["import cv2\n","from google.colab.patches import cv2_imshow\n","\n","OUTPUT_IMAGE_PATH = \"./YOLOX_outputs/yolox_s/vis_res/2022_12_09_12_34_45/南西v3.jpg\" \n","\n","debug_image = cv2.imread(OUTPUT_IMAGE_PATH)\n","cv2_imshow(debug_image)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XL-oSdFrg0Pb"},"outputs":[],"source":["# from PIL import Image\n","\n","# OUTPUT_IMAGE_PATH = \"/content/YOLOX/YOLOX_outputs/nano/vis_res/2021_09_29_17_46_56/000050.jpg\" \n","# Image.open(OUTPUT_IMAGE_PATH)"]},{"cell_type":"markdown","metadata":{"id":"XT2WBgo7jAvR"},"source":["# ONNX出力(Export ONNX Model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5883,"status":"ok","timestamp":1670589075815,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"vHpT0bQBhHzt","outputId":"ff06af91-fca5-43ef-b443-608afa809545"},"outputs":[],"source":["!python tools/export_onnx.py \\\n","    --output-name yolox_s.onnx \\\n","    -n yolox-s \\\n","    -f yolox_s.py \\\n","    -c {MODEL_PATH}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1670589075815,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"ESOnydCZNxAd","outputId":"d4eb8a7a-bb65-4f64-a4d4-f141a0c5d819"},"outputs":[],"source":["%%writefile onnx_inference.py\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","# Copyright (c) Megvii, Inc. and its affiliates.\n","\n","import argparse\n","import os\n","\n","import cv2\n","import numpy as np\n","\n","import onnxruntime\n","\n","from yolox.data.data_augment import preproc as preprocess\n","#from yolox.data.datasets import COCO_CLASSES\n","from yolox.utils import mkdir, multiclass_nms, demo_postprocess, vis\n","\n","COCO_CLASSES = ('back','front') #ここに追記\n","\n","\n","def make_parser():\n","    parser = argparse.ArgumentParser(\"onnxruntime inference sample\")\n","    parser.add_argument(\n","        \"-m\",\n","        \"--model\",\n","        type=str,\n","        default=\"yolox.onnx\",\n","        help=\"Input your onnx model.\",\n","    )\n","    parser.add_argument(\n","        \"-i\",\n","        \"--image_path\",\n","        type=str,\n","        default='test_image.png',\n","        help=\"Path to your input image.\",\n","    )\n","    parser.add_argument(\n","        \"-o\",\n","        \"--output_dir\",\n","        type=str,\n","        default='demo_output',\n","        help=\"Path to your output directory.\",\n","    )\n","    parser.add_argument(\n","        \"-s\",\n","        \"--score_thr\",\n","        type=float,\n","        default=0.3,\n","        help=\"Score threshould to filter the result.\",\n","    )\n","    parser.add_argument(\n","        \"--input_shape\",\n","        type=str,\n","        default=\"640,640\",\n","        help=\"Specify an input shape for inference.\",\n","    )\n","    parser.add_argument(\n","        \"--with_p6\",\n","        action=\"store_true\",\n","        help=\"Whether your model uses p6 in FPN/PAN.\",\n","    )\n","    return parser\n","\n","\n","if __name__ == '__main__':\n","    args = make_parser().parse_args()\n","\n","    input_shape = tuple(map(int, args.input_shape.split(',')))\n","    origin_img = cv2.imread(args.image_path)\n","    img, ratio = preprocess(origin_img, input_shape)\n","\n","    session = onnxruntime.InferenceSession(args.model)\n","\n","    ort_inputs = {session.get_inputs()[0].name: img[None, :, :, :]}\n","    output = session.run(None, ort_inputs)\n","    predictions = demo_postprocess(output[0], input_shape, p6=args.with_p6)[0]\n","\n","    boxes = predictions[:, :4]\n","    scores = predictions[:, 4:5] * predictions[:, 5:]\n","\n","    boxes_xyxy = np.ones_like(boxes)\n","    boxes_xyxy[:, 0] = boxes[:, 0] - boxes[:, 2]/2.\n","    boxes_xyxy[:, 1] = boxes[:, 1] - boxes[:, 3]/2.\n","    boxes_xyxy[:, 2] = boxes[:, 0] + boxes[:, 2]/2.\n","    boxes_xyxy[:, 3] = boxes[:, 1] + boxes[:, 3]/2.\n","    boxes_xyxy /= ratio\n","    dets = multiclass_nms(boxes_xyxy, scores, nms_thr=0.45, score_thr=0.1)\n","    if dets is not None:\n","        final_boxes, final_scores, final_cls_inds = dets[:, :4], dets[:, 4], dets[:, 5]\n","        origin_img = vis(origin_img, final_boxes, final_scores, final_cls_inds,\n","                         conf=args.score_thr, class_names=COCO_CLASSES)\n","\n","    mkdir(args.output_dir)\n","    output_path = os.path.join(args.output_dir, os.path.basename(args.image_path))\n","    cv2.imwrite(output_path, origin_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wc9dkeMh6QXr"},"outputs":[],"source":["TEST_IMAGE_PATH = \"/content/drive/MyDrive/TestImages/南西v3.jpg\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2922,"status":"ok","timestamp":1670589363784,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"q501MZh_jkIv"},"outputs":[],"source":["!python onnx_inference.py \\\n","    -m yolox_s.onnx \\\n","    -i {TEST_IMAGE_PATH} \\\n","    -o ./ \\\n","    -s 0.3 \\\n","    --input_shape 640,640"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488,"output_embedded_package_id":"1fAon0XgvUjg2MJvFAd_oi7VidUAZTPSo"},"executionInfo":{"elapsed":11658,"status":"ok","timestamp":1670589379000,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"uwcQysS_j_yp","outputId":"fddf8ad3-2cd9-4ba2-cde0-c19d16586c7b"},"outputs":[],"source":["from PIL import Image\n","\n","OUTPUT_IMAGE_PATH = \"南西v3.jpg\" \n","Image.open(OUTPUT_IMAGE_PATH)"]},{"cell_type":"markdown","metadata":{"id":"07vb6wwhEAzL"},"source":["# ONNXでバウンディングボックス取得"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1701,"status":"ok","timestamp":1670589431215,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"Y1C79iH7Fxnm"},"outputs":[],"source":["import argparse\n","import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import onnxruntime\n","from google.colab.patches import cv2_imshow\n","from yolox.data.data_augment import preproc as preprocess\n","#from yolox.data.datasets import COCO_CLASSES\n","from yolox.utils import mkdir, multiclass_nms, demo_postprocess, vis\n","\n","COCO_CLASSES = ('back','front') #ここに追記"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1265,"status":"ok","timestamp":1670589437191,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"P6IIr64FEJOS","outputId":"b14994bd-d20d-4205-d4c6-095adb798732"},"outputs":[],"source":["output_dir ='onnx_out'\n","image_path = '/content/drive/MyDrive/TestImages/南西v3.jpg'\n","model = '/content/YOLOX/yolox_s.onnx'\n","    \n","input_shape = (640,640)\n","origin_img = cv2.imread(image_path)\n","img, ratio = preprocess(origin_img, input_shape)\n","session = onnxruntime.InferenceSession(model)\n","ort_inputs = {session.get_inputs()[0].name: img[None, :, :, :]}\n","output = session.run(None, ort_inputs)\n","predictions = demo_postprocess(output[0], input_shape)[0]\n","boxes = predictions[:, :4]\n","scores = predictions[:, 4:5] * predictions[:, 5:]\n","boxes_xyxy = np.ones_like(boxes)\n","boxes_xyxy[:, 0] = boxes[:, 0] - boxes[:, 2]/2.\n","boxes_xyxy[:, 1] = boxes[:, 1] - boxes[:, 3]/2.\n","boxes_xyxy[:, 2] = boxes[:, 0] + boxes[:, 2]/2.\n","boxes_xyxy[:, 3] = boxes[:, 1] + boxes[:, 3]/2.\n","boxes_xyxy /= ratio\n","dets = multiclass_nms(boxes_xyxy, scores, nms_thr=0.45, score_thr=0.5)\n","if dets is not None:\n","    final_boxes, final_scores, final_cls_inds = dets[:, :4], dets[:, 4], dets[:, 5]\n","    origin_img = vis(origin_img, final_boxes, final_scores, final_cls_inds,\n","                      0.3, class_names=COCO_CLASSES)\n","mkdir(output_dir)\n","output_path = os.path.join(output_dir, os.path.basename(image_path))\n","cv2.imwrite(output_path, origin_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"elapsed":315,"status":"ok","timestamp":1670589441258,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"mI3W4nbzG_41","outputId":"4968571b-11db-417d-c198-a72e97cf51d5"},"outputs":[],"source":["result = []\n","[result.extend((final_cls_inds[x],COCO_CLASSES[int(final_cls_inds[x])],final_scores[x],final_boxes[x][0],final_boxes[x][1],final_boxes[x][2],final_boxes[x][3]) for x in range(len(final_scores)))]\n","df = pd.DataFrame(result, columns = ['class-id','class','score','x-min','y-min','x-max','y-max'])\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1670591155038,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"vCbtijXZWRwU","outputId":"cf0e3048-d2d2-47a6-8f4e-22e1288460cb"},"outputs":[],"source":["backc_x = (final_boxes[0][2] + final_boxes[0][0]) / 2\n","backc_y = (final_boxes[0][3] + final_boxes[0][1]) / 2\n","\n","frontc_x = (final_boxes[1][2] + final_boxes[1][0]) / 2\n","frontc_y = (final_boxes[1][3] + final_boxes[1][1]) / 2\n","\n","print(backc_x, backc_y)\n","print(frontc_x, frontc_y)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670668736037,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"AW2nKLrKXMOC","outputId":"c34ef710-70c6-4956-bf84-282e93f49713"},"outputs":[],"source":["import math\n","backc_x = 3168.505859375\n","backc_y = 759.1664428710938\n","\n","frontc_x = 1213.4153442382812\n","frontc_y = 2583.9623413085938\n","\n","front = [frontc_x, frontc_y]\n","center_x = (backc_x + frontc_x) / 2 \n","center_y = (backc_y + frontc_y) / 2\n","center = [center_x, center_y]\n","\n","print(\"front =\", front)\n","print(\"center = \", center)\n","f_moved = [front[0] - center[0], front[1] - center[1]]\n","print(\"f_moved = \", f_moved)\n","\n","#rad = math.atan2(center_y - frontc_y , center_x - frontc_x)\n","rad = math.atan2(-(front[1] - center[1]), front[0] - center[0])\n","\n","\n","print(\"rad = \", rad)\n","\n","deg = rad * (180 / math.pi) \n","\n","print(\"deg =\", deg)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7314,"status":"ok","timestamp":1670660815953,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"1SpHOZK9JLLa","outputId":"814264ef-93be-4d7f-8f8e-f60b74501733"},"outputs":[],"source":["!pip install japanize-matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":445,"status":"ok","timestamp":1670676088224,"user":{"displayName":"みまみま","userId":"08438286750981985853"},"user_tz":-540},"id":"gw7E8xfkcDYn","outputId":"9c2622f4-14e6-4dd4-f50d-469841d02d6d"},"outputs":[],"source":["# python_visualize_vector_2d\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","# 5×5サイズのFigureを作成してAxesを追加\n","fig = plt.figure(figsize = (6, 6))\n","ax = fig.add_subplot(111)\n","\n","# ベクトルを表示\n","# quiver(始点x,始点y,成分x,成分y)\n","ax.quiver(0, 0, f_moved[0], -f_moved[1], color = \"red\", #画像座標系->標準座標系にするためにyに-をつけて反転\n","          angles = 'xy', scale_units = 'xy', scale = 1) \n","\n","plt.scatter(0, 0, label='center') #始点\n","plt.scatter(f_moved[0], -f_moved[1], label='front') #終点\n","\n","\n","plt.legend() #凡例\n","\n","#plt.title('中点からフォーク先端への方向と角度', fontsize=20) #タイトル\n","\n","\n","# 格子点を表示\n","plt.grid()\n","\n","# 軸ラベルの設定\n","ax.set_xlabel(\"x\", fontsize = 16)\n","ax.set_ylabel(\"y\", fontsize = 16)\n","\n","# 軸範囲の設定\n","plt.xlim(-1500, 1500)\n","plt.ylim(-1500, 1500)\n","\n","# x軸とy軸\n","ax.axhline(0, color = \"gray\")\n","ax.axvline(0, color = \"gray\")\n","\n","\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pxuKKcgj6WND"},"source":["# ONNX -> TensorFlow 変換"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKETcM79q3XD"},"outputs":[],"source":["!pip install onnx-tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpiNXqOA6aoU"},"outputs":[],"source":["!onnx-tf convert \\\n","    -i yolox_nano.onnx \\\n","    -o yolox_nano_pb"]},{"cell_type":"markdown","metadata":{"id":"lV7E1wlX6cli"},"source":["# TensorFlow -> TensorFlow-Lite 変換"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Aln_cLaN5G0"},"outputs":[],"source":["!pip install tf-nightly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1zYkqM26alJ"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GI_X2zBhOSmw"},"outputs":[],"source":["%cd /content/YOLOX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdV9QUme-cD0"},"outputs":[],"source":["# ダイナミックレンジ量子化\n","converter = tf.lite.TFLiteConverter.from_saved_model('yolox_nano_pb')\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","tflite_quantized_model = converter.convert()\n","\n","open('yolox_nano_dynamic_range_quantize.tflite', 'wb').write(tflite_quantized_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wgIUaj06aiV"},"outputs":[],"source":["# 半精度浮動小数点数の量子化\n","converter = tf.lite.TFLiteConverter.from_saved_model('yolox_nano_pb')\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_types = [tf.float16]\n","tflite_quantized_model = converter.convert()\n","\n","open('yolox_nano_float16_quantize.tflite', 'wb').write(tflite_quantized_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjmUxifw6aew"},"outputs":[],"source":["# 完全整数量子化\n","import glob\n","import numpy as np\n","from PIL import Image\n","\n","test_image_pathlist = glob.glob('/content/YOLOX-Colaboratory-Training-Sample/01.image/*.jpg')\n","test_image_pathlist = test_image_pathlist[:100]\n","print(len(test_image_pathlist))\n","\n","def representative_dataset():\n","    for test_image_path in test_image_pathlist:\n","        image = np.array(Image.open(test_image_path))\n","        image = image.astype('float32')\n","        image = tf.image.resize(image, (416, 416))\n","        image = image - 127.5\n","        image = image * 0.007843\n","        image = tf.transpose(image, perm=[2, 0, 1])\n","        image = tf.reshape(image, [1, 3, 416, 416])\n","        yield [image]\n","\n","converter = tf.lite.TFLiteConverter.from_saved_model('yolox_nano_pb')\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.representative_dataset = representative_dataset\n","tflite_quantized_model = converter.convert()\n","\n","open('yolox_nano_int8_quantize.tflite', 'wb').write(tflite_quantized_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9cTtsmA6aaY"},"outputs":[],"source":["# 完全整数量子化(入力含む)\n","import glob\n","import numpy as np\n","from PIL import Image\n","\n","test_image_pathlist = glob.glob('/content/YOLOX-Colaboratory-Training-Sample/01.image/*.jpg')\n","test_image_pathlist = test_image_pathlist[:100]\n","print(len(test_image_pathlist))\n","\n","def representative_dataset():\n","    for test_image_path in test_image_pathlist:\n","        image = np.array(Image.open(test_image_path))\n","        image = image.astype('float32')\n","        image = tf.image.resize(image, (416, 416))\n","        image = image - 127.5\n","        image = image * 0.007843\n","        image = tf.transpose(image, perm=[2, 0, 1])\n","        image = tf.reshape(image, [1, 3, 416, 416])\n","        yield [image]\n","\n","converter = tf.lite.TFLiteConverter.from_saved_model('yolox_nano_pb')\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.representative_dataset = representative_dataset\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter.inference_input_type = tf.int8  # or tf.uint8\n","converter.inference_output_type = tf.int8  # or tf.uint8\n","tflite_quantized_model = converter.convert()\n","\n","open('yolox_nano_only_int8_quantize.tflite', 'wb').write(tflite_quantized_model)"]}],"metadata":{"colab":{"collapsed_sections":["pxuKKcgj6WND","lV7E1wlX6cli"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
